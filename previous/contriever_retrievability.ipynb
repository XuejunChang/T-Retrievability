{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfc404f-f2d4-4dfe-8fe0-005cbf4c296c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pyterrier as pt\n",
    "\n",
    "if not pt.java.started():\n",
    "    pt.java.init()\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', False)\n",
    "\n",
    "\n",
    "dataset_name = 'msmarco-passage'\n",
    "eval_ds_name = 'dev'\n",
    "dataset = pt.get_dataset(f'irds:{dataset_name}')\n",
    "eval_dataset = pt.get_dataset(f'irds:{dataset_name}/{eval_ds_name}')\n",
    "topics = eval_dataset.get_topics()\n",
    "qrels = eval_dataset.get_qrels()\n",
    "\n",
    "work_name = \"retrievability-bias\"\n",
    "root_dir = f'/root/{work_name}'\n",
    "nfs_save = f'/nfs/datasets/cxj/{work_name}'\n",
    "if not os.path.exists(nfs_save):\n",
    "    os.makedirs(nfs_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b06e1a-10c5-46ff-aaa2-82c3909ffb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "ed5157c0-c78c-4812-8b6a-6b7a10a04e17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1048578</td>\n",
       "      <td>cost of endless pools swim spa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1048579</td>\n",
       "      <td>what is pcnt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       qid                           query\n",
       "0  1048578  cost of endless pools swim spa\n",
       "1  1048579  what is pcnt                  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145f5e21-79fb-4e04-8490-9c1c26bfe4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dataset.get_corpus_iter(verbose=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a9652b-5213-4b58-8e9b-84a5bb01c344",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0180c22f-19a3-4e49-848d-36618ce4a24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cf3d9e-c422-4122-8e25-966b10f8a81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902323a8-8ae6-4874-bdc1-16bfad3a39e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model and tokenizer\n",
    "model_name = \"facebook/contriever-msmarco\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name).to(device)  # Move model to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093f2346-8c1a-45a0-89e6-0695d9959d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to encode text into dense vectors\n",
    "# def encode_texts(texts):\n",
    "#     # Tokenize and move input tensors to GPU\n",
    "#     inputs = tokenizer.batch_encode_plus(texts, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "#     print(type(inputs))\n",
    "#     print(inputs)\n",
    "#     with torch.no_grad():\n",
    "#         # Generate embeddings and move them back to CPU for further processing\n",
    "#         embeddings = model(**inputs).last_hidden_state.mean(dim=1).cpu()\n",
    "#     return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71195856-9ebb-419c-afe4-6b1bcc752d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_embeddings(inputs):\n",
    "    # Tokenize the inputs\n",
    "    tokenized_inputs = tokenizer(\n",
    "        inputs,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)  # Move tokenized inputs to GPU\n",
    "    \n",
    "    # print(tokenized_inputs)\n",
    "    # Compute embeddings\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**tokenized_inputs)\n",
    "        token_embeddings = model_output.last_hidden_state \n",
    "    \n",
    "    # Aggregate embeddings (e.g., mean pooling)\n",
    "    attention_mask = tokenized_inputs[\"attention_mask\"]  # To ignore padding tokens in the aggregation\n",
    "    masked_embeddings = token_embeddings * attention_mask.unsqueeze(-1)  # Apply attention mask\n",
    "    sum_embeddings = masked_embeddings.sum(dim=1)  # Sum over the sequence length\n",
    "    sum_mask = attention_mask.sum(dim=1).unsqueeze(-1)  # Count non-padding tokens per sequence\n",
    "    inputs_embeddings = sum_embeddings / sum_mask  # Mean pooling: divide by token counts\n",
    "    \n",
    "    # Display final embeddings (shape: batch_size, hidden_dim)\n",
    "    print(inputs_embeddings.shape)  # Example: torch.Size([3, 768])\n",
    "    # print(inputs_embeddings)  # Example: torch.Size([3, 768])\n",
    "    return inputs_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c1e330-fd74-4f96-b489-1681939db092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define query and documents\n",
    "queries = [\"What is the capital of France?\", \"What is the capital of the UK?\", \"What is the capital of China?\", \"What is the capital of the United States?\"]\n",
    "documents = [\n",
    "    \"Paris is the capital city of France.\",\n",
    "    \"France is a country in Europe.\",\n",
    "    \"Berlin is the capital of Germany.\",\n",
    "    \"Madrid is the capital of Spain.\",\n",
    "    \"Paris is known for the Eiffel Tower.\",\n",
    "    \"The Louvre is located in Paris, France.\",\n",
    "    \"London is the capital of the United Kingdom.\",\n",
    "    \"Rome is the capital of Italy.\",\n",
    "    \"Paris is famous for its cuisine.\",\n",
    "    \"The French language is spoken in Paris.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e8b672-7bae-428c-8f7f-7b0ada25f156",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = topics[:10]['query'].to_list()\n",
    "documents = df[:100]['text'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "20249b8a-1ba2-46d9-b892-0e6ca52588fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cost of endless pools swim spa',\n",
       " 'what is pcnt',\n",
       " 'what is pcb waste',\n",
       " 'what is pbis',\n",
       " 'what is paysky',\n",
       " 'what is paydata',\n",
       " 'what is pay range for warehouse specialist in minneapolis',\n",
       " 'what is paula deen s brother',\n",
       " 'what is paul gum disease',\n",
       " 'what is patron']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fe020c-204b-496b-9cdf-ba765cf118fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to numpy (required by scikit-learn)\n",
    "\n",
    "query_embeddings = calc_embeddings(queries).cpu().numpy()\n",
    "document_embeddings = calc_embeddings(documents).cpu().numpy()\n",
    "\n",
    "# Compute cosine similarities (shape: num_queries x num_docs)\n",
    "cos_sim_matrix = cosine_similarity(query_embeddings, document_embeddings)\n",
    "\n",
    "# Display the similarity matrix\n",
    "# print(\"Cosine Similarity Matrix:\")\n",
    "# print(cos_sim_matrix)\n",
    "\n",
    "# Retrieve the top-k documents for each query\n",
    "top_k = 10\n",
    "for i, query_similarities in enumerate(cos_sim_matrix):\n",
    "    top_indices = query_similarities.argsort()[-top_k:][::-1]\n",
    "    print(f\"Query {i}: {queries[i]}\")\n",
    "    print(f\"Top {top_k} documents for Query {i}:\")\n",
    "    top_scores = [query_similarities[idx] for idx in top_indices]\n",
    "    print(top_scores)\n",
    "    for idx in top_indices:\n",
    "        print(f\"  Document {idx}: Score = {query_similarities[idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513157d4-e6f5-419a-94cd-e4f665f06961",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, topics.shape[0],batch_size):\n",
    "    queries = topics[i: i+batch_size]['text'].to_list():\n",
    "    sub_df = df['text'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "04164f92-af98-463c-a1b1-2a1cfede88e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>docno</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The presence of communication amid scientific minds was equally important to the success of the Manhattan Project as scientific intellect was. The only cloud hanging over the impressive achievement of the atomic researchers and engineers is what their success truly meant; hundreds of thousands of innocent lives obliterated.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Manhattan Project and its atomic bomb helped bring an end to World War II. Its legacy of peaceful uses of atomic energy continues to have an impact on history and science.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Essay on The Manhattan Project - The Manhattan Project The Manhattan Project was to see if making an atomic bomb possible. The success of this project would forever change the world forever making it known that something this powerful can be manmade.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Manhattan Project was the name for a project conducted during World War II, to develop the first atomic bomb. It refers specifically to the period of the project from 194 … 2-1946 under the control of the U.S. Army Corps of Engineers, under the administration of General Leslie R. Groves.</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>versions of each volume as well as complementary websites. The first website–The Manhattan Project: An Interactive History–is available on the Office of History and Heritage Resources website, http://www.cfo. doe.gov/me70/history. The Office of History and Heritage Resources and the National Nuclear Security</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                    text  \\\n",
       "0  The presence of communication amid scientific minds was equally important to the success of the Manhattan Project as scientific intellect was. The only cloud hanging over the impressive achievement of the atomic researchers and engineers is what their success truly meant; hundreds of thousands of innocent lives obliterated.   \n",
       "1  The Manhattan Project and its atomic bomb helped bring an end to World War II. Its legacy of peaceful uses of atomic energy continues to have an impact on history and science.                                                                                                                                                         \n",
       "2  Essay on The Manhattan Project - The Manhattan Project The Manhattan Project was to see if making an atomic bomb possible. The success of this project would forever change the world forever making it known that something this powerful can be manmade.                                                                              \n",
       "3  The Manhattan Project was the name for a project conducted during World War II, to develop the first atomic bomb. It refers specifically to the period of the project from 194 … 2-1946 under the control of the U.S. Army Corps of Engineers, under the administration of General Leslie R. Groves.                                    \n",
       "4  versions of each volume as well as complementary websites. The first website–The Manhattan Project: An Interactive History–is available on the Office of History and Heritage Resources website, http://www.cfo. doe.gov/me70/history. The Office of History and Heritage Resources and the National Nuclear Security                   \n",
       "\n",
       "  docno  \n",
       "0  0     \n",
       "1  1     \n",
       "2  2     \n",
       "3  3     \n",
       "4  4     "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[0:5]['text'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "9f3ad046-ee50-41a2-aae8-54fd22034110",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[4,'docno']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fb12b5-4b07-486a-91c0-b807ad54133f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5cb33f-43fd-4fdd-8364-f8b891c8ca27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 4\n",
    "# for i in range(0, len(queries), batch_size):\n",
    "#     batch = queries[i:i+batch_size]\n",
    "#     tokenized_batch = tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "#     print(tokenized_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072de976-5d8c-4cea-9eff-d049e54b4c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Encode query and documents\n",
    "# query_embedding = encode_texts([query])  # Query embedding\n",
    "# document_embeddings = encode_texts(documents)  # Document embeddings\n",
    "\n",
    "# # Compute cosine similarity\n",
    "# cos_sim = cosine_similarity(query_embedding, document_embeddings)\n",
    "# print(cos_sim.shape)\n",
    "# print(cos_sim)\n",
    "\n",
    "# # Rank documents by similarity\n",
    "# top_k = 10\n",
    "# top_indices = cos_sim[0].argsort()[-top_k:][::-1]\n",
    "# # print(type(top_indices))\n",
    "# # print(top_indices.shape)\n",
    "# # print(cos_sim[0].argsort())\n",
    "# # print(cos_sim[0].argsort()[-top_k:])\n",
    "# # print(cos_sim[0].argsort()[-top_k:][::-1])\n",
    "\n",
    "\n",
    "# print(\"Top 10 results:\")\n",
    "# for idx in top_indices:\n",
    "#     print(f\"{documents[idx]} (Score: {cos_sim[0][idx]:.4f})\")\n",
    "    # print(cos_sim[0].shape)\n",
    "    # print(f\"{documents[idx]} Score: {cos_sim[0][idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e87e248-3ecc-4fef-9dfe-03455533ece0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(query_embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b52b447-5ef7-4b66-9a5c-d3bf213878c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = pd.DataFrame(np.array([[1, 2], [3, 4], [5, 6], [7, 8]]), columns=['x', 'y'], dtype=float)\n",
    "print('===a===')\n",
    "print(a)\n",
    "b = pd.DataFrame(np.array([[10, 20], [30, 40]]), columns=['x', 'y'], dtype=float)\n",
    "\n",
    "print('===b===')\n",
    "print(b)\n",
    "# 不重置索引,上下拼接\n",
    "# df = pd.concat([a, b], axis=0, join='inner', ignore_index=True)\n",
    "# print('===df===')\n",
    "# print(df)\n",
    "# m,n = a.shape\n",
    "# m0,n0 = b.shape\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "r = cosine_similarity(b, a)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3911b4-9bea-4f85-bbb6-263db95ac2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(r))\n",
    "print(r.shape)\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "d4add9fa-7a01-4aa8-8c77-a160968e60cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 results:\n",
      "Paris is the capital city of France. (Score: 0.8204)\n",
      "The Louvre is located in Paris, France. (Score: 0.5938)\n",
      "Paris is known for the Eiffel Tower. (Score: 0.5878)\n",
      "London is the capital of the United Kingdom. (Score: 0.5868)\n",
      "The French language is spoken in Paris. (Score: 0.5623)\n",
      "France is a country in Europe. (Score: 0.5574)\n",
      "Rome is the capital of Italy. (Score: 0.5088)\n",
      "Berlin is the capital of Germany. (Score: 0.5031)\n",
      "Paris is famous for its cuisine. (Score: 0.5025)\n",
      "Madrid is the capital of Spain. (Score: 0.4875)\n"
     ]
    }
   ],
   "source": [
    "# Function to encode text into dense vectors\n",
    "def encode_texts(texts):\n",
    "    # Tokenize and move input tensors to GPU\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        # Generate embeddings and move them back to CPU for further processing\n",
    "        embeddings = model(**inputs).last_hidden_state.mean(dim=1).cpu()\n",
    "    return embeddings\n",
    "\n",
    "# Define query and documents\n",
    "query = \"What is the capital of France?\"\n",
    "documents = [\n",
    "    \"Paris is the capital city of France.\",\n",
    "    \"France is a country in Europe.\",\n",
    "    \"Berlin is the capital of Germany.\",\n",
    "    \"Madrid is the capital of Spain.\",\n",
    "    \"Paris is known for the Eiffel Tower.\",\n",
    "    \"The Louvre is located in Paris, France.\",\n",
    "    \"London is the capital of the United Kingdom.\",\n",
    "    \"Rome is the capital of Italy.\",\n",
    "    \"Paris is famous for its cuisine.\",\n",
    "    \"The French language is spoken in Paris.\",\n",
    "]\n",
    "\n",
    "# Encode query and documents\n",
    "query_embedding = encode_texts([query])  # Query embedding\n",
    "document_embeddings = encode_texts(documents)  # Document embeddings\n",
    "\n",
    "# Compute cosine similarity\n",
    "cos_sim = cosine_similarity(query_embedding, document_embeddings)\n",
    "\n",
    "# Rank documents by similarity\n",
    "top_k = 10\n",
    "top_indices = cos_sim[0].argsort()[-top_k:][::-1]\n",
    "\n",
    "# Display top-k results\n",
    "print(\"Top 10 results:\")\n",
    "for idx in top_indices:\n",
    "    print(f\"{documents[idx]} (Score: {cos_sim[0][idx]:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cb95ec-81db-433d-9358-afd22c2a051b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:splade]",
   "language": "python",
   "name": "conda-env-splade-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
