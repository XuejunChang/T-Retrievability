{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f1ab438-79dd-48ad-84f7-0626dbca4bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "msmarco-passage documents: 100%|██████████| 8841823/8841823 [00:48<00:00, 183667.16it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pyterrier as pt\n",
    "\n",
    "if not pt.started():\n",
    "    pt.init()\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "# pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.max_rows', None)\n",
    "# pd.set_option('display.max_colwidth', False)\n",
    "\n",
    "dataset = pt.get_dataset(f'irds:msmarco-passage')\n",
    "df = pd.DataFrame(dataset.get_corpus_iter(verbose=True))\n",
    "\n",
    "eval_dataset = pt.get_dataset(f'irds:msmarco-passage/dev')\n",
    "topics = eval_dataset.get_topics()\n",
    "qrels = eval_dataset.get_qrels()\n",
    "\n",
    "work_name = \"retrievability-bias\"\n",
    "root_dir = f'/root/{work_name}'\n",
    "nfs_save = f'/nfs/datasets/cxj/{work_name}'\n",
    "if not os.path.exists(nfs_save):\n",
    "    os.makedirs(nfs_save)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "250f2cfb-1691-48ed-9217-fcf248948eab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8841823, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bd8bdf-3d1f-4ac3-ae84-32aeb7378e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# model_name = \"facebook/contriever\"  # Contriever model\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model_name = \"facebook/contriever-msmarco\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name).to(device)  # Move model to GPU\n",
    "\n",
    "\n",
    "# Parameters\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model_name = \"facebook/contriever-msmarco\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name).to(device)  # Move model to GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3cd28a-9128-4d37-9deb-087765b54740",
   "metadata": {},
   "outputs": [],
   "source": [
    "shard_size = 10  # Number of documents per shard\n",
    "embedding_dim = 768  # Embedding dimension (Contriever-specific)\n",
    "embedding_dir = \"sharded_embeddings\"\n",
    "os.makedirs(embedding_dir, exist_ok=True)\n",
    "\n",
    "# Load the dataframe with 8.8 million documents\n",
    "df = df[:100]\n",
    "\n",
    "# Function to generate embeddings for a batch of texts\n",
    "def generate_embeddings(texts):\n",
    "    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).cpu().numpy()  # Example: mean pooling\n",
    "\n",
    "\n",
    "# Process the DataFrame in chunks (shards)\n",
    "for i in range(0, len(df), shard_size):\n",
    "    shard_texts = df['text'].iloc[i:i + shard_size].tolist()  # Get text from shard\n",
    "    shard_embeddings = generate_embeddings(shard_texts)\n",
    "\n",
    "    # Save shard embeddings to disk\n",
    "    shard_file = os.path.join(embedding_dir, f\"shard_{i}.npy\")\n",
    "    np.save(shard_file, shard_embeddings)\n",
    "    print(f\"Saved embeddings for shard {i} to {shard_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2082d3b2-3f04-426e-a3ff-7270eab6839b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "# Parameters\n",
    "nlist = 10  # Number of clusters for IVF index\n",
    "faiss_index_dir = \"faiss_indexes\"\n",
    "os.makedirs(faiss_index_dir, exist_ok=True)\n",
    "\n",
    "# Create FAISS index for each shard\n",
    "for shard_file in sorted(os.listdir(embedding_dir)):\n",
    "    if shard_file.endswith(\".npy\"):\n",
    "        # Load shard embeddings\n",
    "        shard_embeddings = np.load(os.path.join(embedding_dir, shard_file))\n",
    "\n",
    "        # Initialize FAISS index\n",
    "        quantizer = faiss.IndexFlatL2(embedding_dim)  # Exact search quantizer\n",
    "        index = faiss.IndexIVFFlat(quantizer, embedding_dim, nlist, faiss.METRIC_L2)\n",
    "\n",
    "        # Train the index on the shard embeddings\n",
    "        if not index.is_trained:\n",
    "            index.train(shard_embeddings)\n",
    "\n",
    "        # Add shard embeddings to the index\n",
    "        index.add(shard_embeddings)\n",
    "\n",
    "        # Save the index to disk\n",
    "        shard_index_file = os.path.join(faiss_index_dir, shard_file.replace(\".npy\", \".index\"))\n",
    "        faiss.write_index(index, shard_index_file)\n",
    "        print(f\"Saved FAISS index for shard {shard_file} to {shard_index_file}\")\n",
    "\n",
    "# Generate query embedding\n",
    "query_text = topics.loc[0,'query']\n",
    "query_inputs = tokenizer(query_text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "with torch.no_grad():\n",
    "    query_embedding = model(**query_inputs).last_hidden_state.mean(dim=1).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a5eccc-70b8-4a2b-96cb-dbe510d67172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search across all shards\n",
    "k = 2  # Top 100 results\n",
    "all_distances, all_indices = [], []\n",
    "\n",
    "for index_file in sorted(os.listdir(faiss_index_dir)):\n",
    "    if index_file.endswith(\".index\"):\n",
    "        # Load the FAISS index\n",
    "        index = faiss.read_index(os.path.join(faiss_index_dir, index_file))\n",
    "\n",
    "        # Search the index\n",
    "        distances, indices = index.search(query_embedding, k)\n",
    "        print(distances)\n",
    "        print(indices)\n",
    "        \n",
    "        all_distances.append(distances)\n",
    "        all_indices.append(indices)\n",
    "\n",
    "# Combine results from all shards\n",
    "all_distances = np.hstack(all_distances)  # Combine distances\n",
    "all_indices = np.hstack(all_indices)  # Combine indices\n",
    "\n",
    "# Sort to get the top 100 results globally\n",
    "sorted_indices = np.argsort(all_distances[0])[:k]\n",
    "final_indices = all_indices[0][sorted_indices]\n",
    "final_distances = all_distances[0][sorted_indices]\n",
    "\n",
    "print(\"Top 100 document indices:\", final_indices)\n",
    "print(\"Top 100 distances:\", final_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489c5ff3-a721-4a8e-8478-f6956186cf6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:splade]",
   "language": "python",
   "name": "conda-env-splade-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
