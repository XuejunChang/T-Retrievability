{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfc404f-f2d4-4dfe-8fe0-005cbf4c296c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "import pyterrier as pt\n",
    "if not pt.java.started():\n",
    "    pt.java.init()\n",
    "\n",
    "import ir_datasets\n",
    "import ir_measures\n",
    "from ir_measures import * # imports all supported measures, e.g., AP, nDCG, RR, P\n",
    "import statistics\n",
    "\n",
    "import os\n",
    "os.environ[\"PIP_ROOT_USER_ACTION\"] = \"ignore\"\n",
    "import glob\n",
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0ece66-f982-4675-a9ba-e32723bbcd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_name = 'msmarco-passage'\n",
    "# dataset = pt.get_dataset(f'irds:{dataset_name}')\n",
    "\n",
    "dataset = pt.get_dataset(f'irds:msmarco-passage')\n",
    "# df_dataset = pd.DataFrame(dataset.get_corpus_iter(verbose=True))\n",
    "eval_dev = pt.get_dataset(f'irds:msmarco-passage/dev')\n",
    "dev_topics = eval_dev.get_topics()\n",
    "# qrels = eval_dev.get_qrels()\n",
    "\n",
    "dev_eval = ir_datasets.load(\"msmarco-passage/dev\")\n",
    "# topics = pd.DataFrame(eval.queries_iter())\n",
    "dev_qrels = pd.DataFrame(dev_eval.qrels_iter())\n",
    "\n",
    "\n",
    "\n",
    "dl19 = pt.get_dataset('irds:msmarco-passage/trec-dl-2019')\n",
    "dl19_topics = dl19.get_topics()\n",
    "# dl19_qrels = dl19.get_qrels()\n",
    "\n",
    "dl20 = pt.get_dataset('irds:msmarco-passage/trec-dl-2020')\n",
    "dl20_topics = dl20.get_topics()\n",
    "# dl20_qrels = dl20.get_qrels()\n",
    "\n",
    "dl19_eval = ir_datasets.load(\"msmarco-passage/trec-dl-2019\")\n",
    "# topics = pd.DataFrame(eval.queries_iter())\n",
    "dl19_qrels = pd.DataFrame(dl19_eval.qrels_iter())\n",
    "\n",
    "\n",
    "dl20_eval = ir_datasets.load(\"msmarco-passage/trec-dl-2020\")\n",
    "# topics = pd.DataFrame(eval.queries_iter())\n",
    "dl20_qrels = pd.DataFrame(dl20_eval.qrels_iter())\n",
    "\n",
    "dl1920_topics = pd.concat([dl19_topics, dl20_topics], ignore_index=True)\n",
    "dl1920_qrels = pd.concat([dl19_qrels, dl20_qrels], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52b0920-ad48-41c8-8ead-d683a3db05e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gini(v):\n",
    "    v = np.array(v)\n",
    "    bins = np.linspace(0., 100., 11)\n",
    "    total = float(np.sum(v))\n",
    "    yvals = [0]\n",
    "    for b in bins[1:]:\n",
    "        bin_vals = v[v <= np.percentile(v, b)]\n",
    "        bin_fraction = (np.sum(bin_vals) / total) * 100.0\n",
    "        yvals.append(bin_fraction)\n",
    "    # perfect equality area\n",
    "    pe_area = np.trapz(bins, x=bins)\n",
    "    # lorenz area\n",
    "    lorenz_area = np.trapz(yvals, x=bins)\n",
    "    gini_val = (pe_area - lorenz_area) / float(pe_area)\n",
    "    return gini_val\n",
    "\n",
    "def calc_stats_v2(modelname,df,scoredF, topics,qrels):\n",
    "    qids_to_keep = topics['qid'].to_list()\n",
    "    mask = np.logical_or.reduce([df[\"qid\"] == val for val in qids_to_keep])\n",
    "    df_filtered = df[mask]\n",
    "    grouped = df_filtered.groupby(\"docno\")[['r_score']].sum().reset_index()\n",
    "    grouped.to_csv(scoredF, index=False)\n",
    "\n",
    "    # if not os.path.exists(scoredF):\n",
    "    #     # topics['qid'] = topics['qid'].astype(str)\n",
    "    #     qids_to_keep = topics['qid'].to_list()\n",
    "    #     mask = np.logical_or.reduce([df[\"qid\"] == val for val in qids_to_keep])\n",
    "    #     df_filtered = df[mask]\n",
    "    #     grouped = df_filtered.groupby(\"docno\")[['r_score']].sum().reset_index()\n",
    "    #     grouped.to_csv(scoredF,index=False)\n",
    "    # else:\n",
    "    #     grouped = pd.read_csv(scoredF, index_col=0).reset_index()\n",
    "\n",
    "    scores = grouped['r_score'].to_list()\n",
    "\n",
    "    print('start statistics')\n",
    "    mean = statistics.mean(scores)\n",
    "    std_dev = statistics.stdev(scores)\n",
    "    gini_value = Gini(scores)\n",
    "\n",
    "    df2 = pd.DataFrame()\n",
    "    df2['query_id'] = df['qid'].astype(str)\n",
    "    df2['doc_id'] = df['docno'].astype(str)\n",
    "    df2['score'] = df['score']\n",
    "    m = ir_measures.calc_aggregate([nDCG@10, RR], qrels, df2)\n",
    "    print(mean, std_dev, gini_value, m[nDCG@10], m[RR])\n",
    "    return mean, std_dev, gini_value, m[nDCG@10], m[RR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f522e8-d835-414d-bd2e-cf75d1c830be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f2153c-9d21-4623-a48b-21cd276b1d2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c57492-6a9e-40ae-813d-df988a215bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_topics['cluster'] = 0\n",
    "dev_all = dev_topics.groupby('cluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b87ece-ecd8-4666-8dd4-94bbb3d1bd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl1920_topics['cluster'] = 0\n",
    "dl1920_all = dl1920_topics.groupby('cluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8ace81-a5e0-4dc5-967c-8503a6f2be0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_topics_sampled = pd.read_csv('./results/smapled_dev_queries_50.csv', index_col=0).reset_index() # 2000 clusters with each 50 queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb813a9-5914-4548-bda7-4dd12108c96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_2000_grps = dev_topics_sampled.groupby('cluster')\n",
    "# print(len(grouped.groups.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2016a292-347f-44f3-b9f6-0e1a649ed490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dl1920_grouped.size().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c29987-52d4-402a-8109-6c9948b60d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/nfs/resources/cxj/retrievability-bias/tctcolbert/df_tctcolbert_rscore_dl1920_0.csv', index_col=0).reset_index()\n",
    "df['qid'] = df['qid'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d594416b-edb5-407c-b9db-aeb528d05dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7508876-aa33-4082-b310-a78ce13fdc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster_id, queries_df in dl1920_all:\n",
    "    print(queries_df.shape)\n",
    "    scoredF = f'/root/tctcolbert_dev_all_T0_G0.csv'\n",
    "    if os.path.exists(scoredF):\n",
    "        os.remove(scoredF)\n",
    "\n",
    "    mean, std_dev, gini_value, ndcg10, rr = calc_stats_v2('tctcolbert',df,scoredF,queries_df[:5], dl1920_qrels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a31ff1f-b3b2-414d-a7d3-3392b032915c",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = f'/root/retrievability-bias'\n",
    "nfs_dir = f'/nfs/datasets/cxj/retrievability-bias-from_resources_ok'\n",
    "\n",
    "# all_topics = ['dl1920_all', 'dev_all', 'dev_2000_grps']\n",
    "all_topics = ['dev_2000_grps']\n",
    "pt.tqdm.pandas()\n",
    "\n",
    "for eval_topics in all_topics:\n",
    "    # for modelname in ['bm25', 'bm25_monot5', 'splade', 'tctcolbert','bm25_tctcolbert']:\n",
    "    for modelname in ['tctcolbert','bm25_tctcolbert']:\n",
    "        this_model_res = []\n",
    "        for threshold in [0, 30, 60, 90]:\n",
    "            \"\"\"\n",
    "            Calc retrievability score for each doc\n",
    "            \"\"\" \n",
    "            print(f'start {modelname} ----> {eval_topics} ----> threshold {threshold}')\n",
    "            rscore_csv = f'/nfs/resources/cxj/retrievability-bias/{modelname}/df_{modelname}_rscore_{eval_topics}_{threshold}.csv'\n",
    "            if os.path.exists(rscore_csv):\n",
    "                df = pd.read_csv(rscore_csv, index_col=0).reset_index()\n",
    "            else:\n",
    "                origin_topics = eval_topics.split('_')[0]\n",
    "                csv = f'/nfs/resources/cxj/retrievability-bias/{modelname}/df_{modelname}_{origin_topics}_{threshold}.csv'\n",
    "                df = pd.read_csv(csv, index_col=0).reset_index()\n",
    "                df['r_score'] = df['rank'].progress_apply(lambda x: 100 / np.log(x + 2))\n",
    "                print(f'saving {rscore_csv}')\n",
    "                df.to_csv(rscore_csv, index=False)\n",
    "                print(f'done')\n",
    "    \n",
    "            \"\"\"\n",
    "            Calc stats for each group \n",
    "            \"\"\"   \n",
    "            if eval_topics == 'dev_all':\n",
    "                grouped = dev_all\n",
    "                qrels = dev_qrels\n",
    "            elif eval_topics == 'dl1920_all':\n",
    "                grouped = dl1920_all\n",
    "                qrels = dl1920_qrels\n",
    "            else:\n",
    "                grouped = dev_2000_grps\n",
    "                qrels = dev_qrels\n",
    "            \n",
    "            res = []\n",
    "            for cluster_id, queries_df in grouped:\n",
    "                print(f'Calc stats {modelname} ----> {eval_topics} ----> threshold {threshold} --> cluster_id = {cluster_id}')\n",
    "                scoredF = f'{nfs_dir}/{modelname}/groups/{modelname}_{eval_topics}_T{threshold}_G{cluster_id}.csv'\n",
    "                df['qid'] = df['qid'].astype(str)\n",
    "                mean, std_dev, gini_value, nDCG10, rr = calc_stats_v2(modelname, df, scoredF, queries_df, qrels)\n",
    "                group_res = [modelname, threshold, cluster_id, mean, std_dev, gini_value, nDCG10, rr]\n",
    "                res.append(group_res)\n",
    "    \n",
    "            \"\"\"\n",
    "            put into a dataframe for all cluster_ids \n",
    "            \"\"\" \n",
    "            print(f'merge for each threshold for {modelname} ----> {eval_topics} ----> threshold {threshold}')\n",
    "            df_threshold = pd.DataFrame(res, columns=['modelname', 'threshold', 'cluster_id', 'mean', 'std', 'gini', 'nDCG@10', 'RR'])\n",
    "            res_csv = f'{nfs_dir}/{modelname}/groups/result_{eval_topics}_T{threshold}_allgroups.csv'\n",
    "            print(f'saving {res_csv}')\n",
    "            df_threshold.to_csv(res_csv, index=False)\n",
    "            print('done')\n",
    "    \n",
    "            \"\"\"\n",
    "            Calc results for this threshold \n",
    "            \"\"\"  \n",
    "            print(f'Calc ginis for each threshold for {modelname} ----> {eval_topics} ----> threshold {threshold}')\n",
    "            ginis = df_threshold['gini']\n",
    "            min_gini, mean_gini, max_gini = ginis.min(), ginis.mean(), ginis.max()\n",
    "            nDCG10 = df_threshold['nDCG@10'].mean()\n",
    "            rr = df_threshold['RR'].mean()\n",
    "            this_model_res.append([modelname, threshold, min_gini, mean_gini, max_gini, nDCG10, rr])\n",
    "    \n",
    "        \"\"\"\n",
    "        Merge all thresholds for this model.\n",
    "        \"\"\"\n",
    "        print(f'Merge into one file for {modelname} ----> {eval_topics}')\n",
    "        res_df = pd.DataFrame(this_model_res, columns=['modelname', 'threshold', 'min_gini', 'mean_gini', 'max_gini','nDCG@10', 'RR'])\n",
    "        res_csv = f'{nfs_dir}/allresults/result_{modelname}_{eval_topics}_stats.csv'\n",
    "        print(f'saving {res_csv}')\n",
    "        res_df.to_csv(res_csv, index=False)\n",
    "        os.system(f'cp -r {res_csv} /nfs/primary/retrievability-bias/results/')\n",
    "        print(f'copied {res_csv}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0335ce35-e45d-4527-96ee-aa1c3b58d73d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9693ce-77de-44dd-bfdf-d1241acb0604",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3753f0-e1e2-44c3-b39b-5e811e8f8cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# Calc stats of each group for each threshold\n",
    "# \"\"\"\n",
    "\n",
    "# nfs_dir = f'/nfs/resources/cxj/retrievability-bias'\n",
    "# pt.tqdm.pandas()\n",
    "\n",
    "# # for modelname in ['bm25', 'bm25_monot5', 'splade', 'colbert', 'bm25_colbert']:\n",
    "# for modelname in ['tctcolbert', 'bm25_tctcolbert']:\n",
    "#     for threshold in [0, 30, 60, 90]:\n",
    "    \n",
    "#         csv2 = f'/nfs/resources/cxj/retrievability-bias/{modelname}/df_{modelname}_rscore_{threshold}.csv'\n",
    "#         print(f'reading {csv2}')\n",
    "#         df = pd.read_csv(csv2, index_col=0).reset_index()\n",
    "        \n",
    "#         res = []\n",
    "#         for cluster_id, queries_df in grouped:\n",
    "#             print(f'start {modelname} ----> threshold {threshold} --> cluster_id = {cluster_id}')\n",
    "#             scoredF = f'{nfs_dir}/{modelname}/groups/{modelname}_T{threshold}_G{cluster_id}.csv'\n",
    "#             mean, std, gini = calc_stats_v2(modelname, df, scoredF, queries_df)\n",
    "#             group_res = [modelname, threshold, cluster_id, mean, std, gini]\n",
    "#             print(group_res)\n",
    "#             res.append(group_res)\n",
    "    \n",
    "#         print(f'start creating df per threshold')\n",
    "#         df_threshold = pd.DataFrame(res, columns=['modelname', 'threshold', 'cluster_id', 'mean', 'std', 'gini'])\n",
    "#         res_csv = f'{nfs_dir}/{modelname}/groups/result_T{threshold}_allgroups.csv'\n",
    "#         print(f'saving {res_csv}')\n",
    "#         df_threshold.to_csv(res_csv, index=False)\n",
    "#         print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30282977-a198-4a3c-8d4e-667e45710c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# mean,man, min gini for each threshold\n",
    "# \"\"\"\n",
    "\n",
    "# nfs_dir = f'/nfs/resources/cxj/retrievability-bias'\n",
    "# root_dir = f'/nfs/primary/retrievability-bias'\n",
    "# # for modelname in ['bm25', 'bm25_monot5', 'splade', 'colbert', 'bm25_colbert']:\n",
    "# for modelname in ['tctcolbert', 'bm25_tctcolbert']:\n",
    "# res = []\n",
    "#     for threshold in [0, 30, 60, 90]:\n",
    "#         res_csv = f'{nfs_dir}/{modelname}/groups/result_T{threshold}_allgroups.csv'\n",
    "#         df_threshold = pd.read_csv(res_csv, index_col=0).reset_index()\n",
    "#         ginis = df_threshold['gini']\n",
    "#         min_gini, mean_gini, max_gini = ginis.min(), ginis.mean(), ginis.max()\n",
    "#         res.append([modelname, threshold, min_gini, mean_gini, max_gini])\n",
    "        \n",
    "#     res_df = pd.DataFrame(res, columns=['modelname', 'threshold', 'min_gini', 'mean_gini', 'max_gini'])\n",
    "#     res_csv = f'{nfs_dir}/result_{modelname}_stats.csv'\n",
    "#     print(f'saving {res_csv}')\n",
    "#     res_df.to_csv(res_csv, index=False)\n",
    "#     os.system(f'cp -r {res_csv} {root_dir}/results/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749d98df-b24a-4286-a5ba-dbe99bffc698",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1d2cff-2c2d-49c9-9f4d-727d249ff19f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3.10.15]",
   "language": "python",
   "name": "conda-env-py3.10.15-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
